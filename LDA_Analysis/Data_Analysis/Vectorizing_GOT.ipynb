{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with various text analysis techniques for the Game of Thrones reddit data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "got = pickle.load( open( \"./Data/final_got_monday.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153179, 7231)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer = 'word', encoding = 'string',  strip_accents = 'unicode',\n",
    "                                   stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "count_got = count_vectorizer.fit_transform(got)\n",
    "count_got.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_got_pd = pd.DataFrame(count_got)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA/PCA/SVD ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17006132, 0.10962433, 0.10199264])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "#  Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "lsa_count = TruncatedSVD(3)\n",
    "count_doc_topic = lsa_count.fit_transform(count_got)\n",
    "lsa_count.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abdicate</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>ability</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abolish</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yunkai</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 7231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             abandon  abandoned  abandonment  abdicate  abdomen  ability  \\\n",
       "component_1    0.000      0.000          0.0       0.0      0.0    0.001   \n",
       "component_2    0.002      0.002          0.0       0.0      0.0    0.007   \n",
       "component_3   -0.000     -0.000         -0.0      -0.0     -0.0   -0.001   \n",
       "\n",
       "             ablaze   able  aboard  abolish  ...  youtube  yunkai  yup   zero  \\\n",
       "component_1   0.000  0.002     0.0      0.0  ...      0.0   0.000  0.0  0.000   \n",
       "component_2   0.001  0.025     0.0      0.0  ...      0.0   0.002  0.0  0.003   \n",
       "component_3  -0.000 -0.002    -0.0     -0.0  ...     -0.0  -0.000 -0.0 -0.000   \n",
       "\n",
       "             zig  zimmer  zombie  zone  zoned  zoom  \n",
       "component_1  0.0     0.0   0.000   0.0    0.0   0.0  \n",
       "component_2  0.0     0.0   0.004   0.0    0.0   0.0  \n",
       "component_3 -0.0     0.0  -0.000  -0.0   -0.0  -0.0  \n",
       "\n",
       "[3 rows x 7231 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "got_count_topic_word = pd.DataFrame(lsa_count.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", 'component_3'],\n",
    "             columns = count_vectorizer.get_feature_names())\n",
    "got_count_topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "plot, armor, jon, night, dany, bran, like, arya, cersei, people, think, battle, time, way, throne\n",
      "\n",
      "Topic  1\n",
      "jon, night, dany, bran, arya, like, cersei, think, people, battle, time, throne, dead, dragon, way\n",
      "\n",
      "Topic  2\n",
      "hold, door, armor, plot, hodor, tumblin, wylis, beer, vi, bus, stargaryen, rope, oak, furdik, reconnaissance\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_count, count_vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abdicate</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>ability</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abolish</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yunkai</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>watch wait curious see watch c...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picked dragon glass dagger fou...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book question man without face...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enough time binge list must wa...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book question faceless man old...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need subscription watch new go...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watching six wonder time robin...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samwell tarly daenerys sure so...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got death bingo made death bin...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 7231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   abandon  abandoned  abandonment  abdicate  \\\n",
       "watch wait curious see watch c...        0          0            0         0   \n",
       "picked dragon glass dagger fou...        0          0            0         0   \n",
       "book question man without face...        0          0            0         0   \n",
       "got...                                   0          0            0         0   \n",
       "enough time binge list must wa...        0          0            0         0   \n",
       "book question faceless man old...        0          0            0         0   \n",
       "need subscription watch new go...        0          0            0         0   \n",
       "watching six wonder time robin...        0          0            0         0   \n",
       "samwell tarly daenerys sure so...        0          0            0         0   \n",
       "got death bingo made death bin...        0          0            0         0   \n",
       "\n",
       "                                   abdomen  ability  ablaze  able  aboard  \\\n",
       "watch wait curious see watch c...        0        0       0     0       0   \n",
       "picked dragon glass dagger fou...        0        0       0     0       0   \n",
       "book question man without face...        0        0       0     0       0   \n",
       "got...                                   0        0       0     0       0   \n",
       "enough time binge list must wa...        0        0       0     1       0   \n",
       "book question faceless man old...        0        0       0     0       0   \n",
       "need subscription watch new go...        0        0       0     0       0   \n",
       "watching six wonder time robin...        0        0       0     0       0   \n",
       "samwell tarly daenerys sure so...        0        0       0     0       0   \n",
       "got death bingo made death bin...        0        0       0     0       0   \n",
       "\n",
       "                                   abolish  ...  youtube  yunkai  yup  zero  \\\n",
       "watch wait curious see watch c...        0  ...        0       0    0     0   \n",
       "picked dragon glass dagger fou...        0  ...        0       0    0     0   \n",
       "book question man without face...        0  ...        0       0    0     0   \n",
       "got...                                   0  ...        0       0    0     0   \n",
       "enough time binge list must wa...        0  ...        0       1    0     1   \n",
       "book question faceless man old...        0  ...        0       0    0     0   \n",
       "need subscription watch new go...        0  ...        0       0    0     0   \n",
       "watching six wonder time robin...        0  ...        0       0    0     0   \n",
       "samwell tarly daenerys sure so...        0  ...        0       0    0     0   \n",
       "got death bingo made death bin...        0  ...        0       0    0     0   \n",
       "\n",
       "                                   zig  zimmer  zombie  zone  zoned  zoom  \n",
       "watch wait curious see watch c...    0       0       0     0      0     0  \n",
       "picked dragon glass dagger fou...    0       0       0     0      0     0  \n",
       "book question man without face...    0       0       0     0      0     0  \n",
       "got...                               0       0       0     0      0     0  \n",
       "enough time binge list must wa...    0       0       0     0      0     0  \n",
       "book question faceless man old...    0       0       0     0      0     0  \n",
       "need subscription watch new go...    0       0       0     0      0     0  \n",
       "watching six wonder time robin...    0       0       0     0      0     0  \n",
       "samwell tarly daenerys sure so...    0       0       0     0      0     0  \n",
       "got death bingo made death bin...    0       0       0     0      0     0  \n",
       "\n",
       "[10 rows x 7231 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "got_label = [e[:30]+\"...\" for e in got]\n",
    "pd.DataFrame(count_got.toarray(), index=got_label, columns=count_vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nmf_model = NMF(3)\n",
    "count_nmf_topic = count_nmf_model.fit_transform(count_got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_topic_word_got = pd.DataFrame(count_nmf_model.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", 'component_3'],\n",
    "             columns = count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "plot, armor, like, story, got, battle, writing, series, people, main, way, really, game, time, point\n",
      "\n",
      "Topic  1\n",
      "jon, night, dany, bran, arya, like, cersei, think, people, battle, time, throne, dead, dragon, way\n",
      "\n",
      "Topic  2\n",
      "hold, door, hodor, bran, dead, time, past, jaime, dothraki, undead, castle, wildfire, meera, unsullied, winterfell\n"
     ]
    }
   ],
   "source": [
    "display_topics(count_nmf_model, count_vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = pd.DataFrame(count_doc_topic.round(10),\n",
    "             index = got_label,\n",
    "             columns = [\"component_1\",\"component_2\", 'component_3', 'component_4', 'component_5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(count_got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-20 12:31:30,953 : INFO : topic #0 (0.200): 0.003*\"assume sex\" + 0.002*\"cersei shelter\" + 0.002*\"arya chase\" + 0.002*\"certainly justify\" + 0.002*\"bell audience\" + 0.002*\"bell favorite\" + 0.002*\"bell concentrated\" + 0.002*\"bell braid\" + 0.002*\"briene south\" + 0.002*\"car dothraki\"\n",
      "2019-05-20 12:31:30,966 : INFO : topic #1 (0.200): 0.003*\"assassin darth\" + 0.003*\"better choice\" + 0.002*\"bring martha\" + 0.002*\"added related\" + 0.002*\"body burnt\" + 0.002*\"burning conquering\" + 0.002*\"cersei board\" + 0.001*\"believe happening\" + 0.001*\"bulk wight\" + 0.001*\"based individual\"\n",
      "2019-05-20 12:31:30,978 : INFO : topic #2 (0.200): 0.003*\"cersei tragically\" + 0.003*\"cersei layer\" + 0.003*\"big fleet\" + 0.003*\"big filler\" + 0.002*\"bran pussy\" + 0.002*\"cersei hollywood\" + 0.002*\"bigger course\" + 0.002*\"addition happy\" + 0.002*\"alternative like\" + 0.002*\"believe ala\"\n",
      "2019-05-20 12:31:30,989 : INFO : topic #3 (0.200): 0.003*\"better analysis\" + 0.002*\"actor cool\" + 0.002*\"broad secret\" + 0.002*\"change completely\" + 0.002*\"bean watch\" + 0.002*\"charisma make\" + 0.002*\"care forgetting\" + 0.001*\"absolute nose\" + 0.001*\"absolute snack\" + 0.001*\"absence\"\n",
      "2019-05-20 12:31:30,999 : INFO : topic #4 (0.200): 0.003*\"body able\" + 0.003*\"big boost\" + 0.002*\"candidate rule\" + 0.002*\"canon end\" + 0.002*\"arya left\" + 0.002*\"bran ironborn\" + 0.002*\"bran ice\" + 0.002*\"abandon arc\" + 0.001*\"ash world\" + 0.001*\"causing unite\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"assume sex\" + 0.002*\"cersei shelter\" + 0.002*\"arya chase\" + 0.002*\"certainly justify\" + 0.002*\"bell audience\" + 0.002*\"bell favorite\" + 0.002*\"bell concentrated\" + 0.002*\"bell braid\" + 0.002*\"briene south\" + 0.002*\"car dothraki\"'),\n",
       " (1,\n",
       "  '0.003*\"assassin darth\" + 0.003*\"better choice\" + 0.002*\"bring martha\" + 0.002*\"added related\" + 0.002*\"body burnt\" + 0.002*\"burning conquering\" + 0.002*\"cersei board\" + 0.001*\"believe happening\" + 0.001*\"bulk wight\" + 0.001*\"based individual\"'),\n",
       " (2,\n",
       "  '0.003*\"cersei tragically\" + 0.003*\"cersei layer\" + 0.003*\"big fleet\" + 0.003*\"big filler\" + 0.002*\"bran pussy\" + 0.002*\"cersei hollywood\" + 0.002*\"bigger course\" + 0.002*\"addition happy\" + 0.002*\"alternative like\" + 0.002*\"believe ala\"'),\n",
       " (3,\n",
       "  '0.003*\"better analysis\" + 0.002*\"actor cool\" + 0.002*\"broad secret\" + 0.002*\"change completely\" + 0.002*\"bean watch\" + 0.002*\"charisma make\" + 0.002*\"care forgetting\" + 0.001*\"absolute nose\" + 0.001*\"absolute snack\" + 0.001*\"absence\"'),\n",
       " (4,\n",
       "  '0.003*\"body able\" + 0.003*\"big boost\" + 0.002*\"candidate rule\" + 0.002*\"canon end\" + 0.002*\"arya left\" + 0.002*\"bran ironborn\" + 0.002*\"bran ice\" + 0.002*\"abandon arc\" + 0.001*\"ash world\" + 0.001*\"causing unite\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_count.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF Vectorizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sklearn, it's VERY similar to how we did CountVectorizer\n",
    "tf_idf_vectorizer = TfidfVectorizer(  \n",
    "                                   stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "got_tfidf = tf_idf_vectorizer.fit_transform(got)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA/PCA/SVD ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00799442, 0.01226075])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "#  Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "tf_idf_lsa = TruncatedSVD(2)\n",
    "tf_idf_doc_topic = tf_idf_lsa.fit_transform(got_tfidf)\n",
    "tf_idf_lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abdicate</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>ability</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abolish</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yunkai</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 7231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             abandon  abandoned  abandonment  abdicate  abdomen  ability  \\\n",
       "component_1      0.0        0.0          0.0       0.0      0.0     0.01   \n",
       "component_2     -0.0       -0.0         -0.0       0.0     -0.0    -0.00   \n",
       "\n",
       "             ablaze  able  aboard  abolish  ...  youtube  yunkai  yup  zero  \\\n",
       "component_1     0.0  0.02     0.0      0.0  ...      0.0     0.0  0.0   0.0   \n",
       "component_2    -0.0 -0.01    -0.0      0.0  ...      0.0    -0.0 -0.0  -0.0   \n",
       "\n",
       "             zig  zimmer  zombie  zone  zoned  zoom  \n",
       "component_1  0.0     0.0    0.01   0.0    0.0   0.0  \n",
       "component_2 -0.0    -0.0   -0.00   0.0   -0.0  -0.0  \n",
       "\n",
       "[2 rows x 7231 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_topic_word = pd.DataFrame(tf_idf_lsa.components_.round(2),\n",
    "             index = [\"component_1\",\"component_2\"],\n",
    "             columns = tf_idf_vectorizer.get_feature_names())\n",
    "tf_idf_topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran = tf_idf_topic_word.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "game          0.68\n",
       "throne        0.59\n",
       "iron          0.05\n",
       "watch         0.02\n",
       "theme         0.02\n",
       "petition      0.01\n",
       "premiere      0.01\n",
       "cover         0.01\n",
       "style         0.01\n",
       "sit           0.01\n",
       "song          0.01\n",
       "edition       0.01\n",
       "fan           0.01\n",
       "official      0.01\n",
       "pool          0.01\n",
       "inspired      0.01\n",
       "remake        0.01\n",
       "trailer       0.01\n",
       "video         0.01\n",
       "review        0.01\n",
       "promo         0.01\n",
       "win           0.01\n",
       "finale        0.01\n",
       "recap         0.01\n",
       "fireproof    -0.00\n",
       "fleshed      -0.00\n",
       "flawed       -0.00\n",
       "flawless     -0.00\n",
       "flea         -0.00\n",
       "fleabottom    0.00\n",
       "              ... \n",
       "sansa        -0.03\n",
       "landing      -0.03\n",
       "got          -0.03\n",
       "jaime        -0.03\n",
       "maybe        -0.03\n",
       "thought      -0.03\n",
       "snow         -0.03\n",
       "way          -0.03\n",
       "stark        -0.03\n",
       "theory       -0.03\n",
       "time         -0.04\n",
       "dragon       -0.04\n",
       "white        -0.04\n",
       "tyrion       -0.04\n",
       "people       -0.04\n",
       "really       -0.04\n",
       "going        -0.04\n",
       "dead         -0.04\n",
       "winterfell   -0.04\n",
       "know         -0.04\n",
       "battle       -0.05\n",
       "kill         -0.05\n",
       "cersei       -0.06\n",
       "think        -0.07\n",
       "like         -0.08\n",
       "dany         -0.09\n",
       "arya         -0.12\n",
       "jon          -0.13\n",
       "night        -0.18\n",
       "bran         -0.19\n",
       "Name: component_2, Length: 7231, dtype: float64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran['component_2'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "throne, night, game, jon, bran, like, arya, dany, think, got, cersei, people, know, end, battle\n",
      "\n",
      "Topic  1\n",
      "game, throne, iron, watch, theme, review, recap, sit, video, premiere, cover, remake, finale, petition, win\n"
     ]
    }
   ],
   "source": [
    "display_topics(tf_idf_lsa, tf_idf_vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### NMF ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_nmf_model = NMF(2)\n",
    "nmf_doc_topic = tf_idf_nmf_model.fit_transform(got_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_topic_word_got = pd.DataFrame(tf_idf_nmf_model.components_.round(2),\n",
    "             index = [\"component_1\",\"component_2\"],\n",
    "             columns = tf_idf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "night, jon, bran, arya, like, dany, think, got, cersei, people, know, battle, time, kill, theory, end, dragon, really, going, tyrion, dead, daenerys, way, winterfell, scene, snow, sansa, white, die, thought, good, jaime, army, death, landing, make, stark, long, ending, queen\n",
      "\n",
      "Topic  1\n",
      "game, throne, iron, watch, end, ending, theme, final, series, finale, new, watching, win, sit, video, fan, recap, review, best, got, theory, premiere, song, prediction, cover, remake, petition, trailer, tonight, day, pool, want, daenerys, music, play, live, claim, real, week, favorite\n"
     ]
    }
   ],
   "source": [
    "display_topics(tf_idf_nmf_model, tf_idf_vectorizer.get_feature_names(), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(got_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in tf_idf_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-19 16:38:25,914 : INFO : topic #0 (0.143): 0.005*\"blah foreshadowing\" + 0.004*\"burnt brandon\" + 0.004*\"belong saying\" + 0.004*\"basically attempted\" + 0.004*\"beat make\" + 0.003*\"army sub\" + 0.003*\"additional safety\" + 0.003*\"alot shitting\" + 0.002*\"broken seen\" + 0.002*\"away writing\"\n",
      "2019-05-19 16:38:25,939 : INFO : topic #1 (0.143): 0.004*\"bran tasked\" + 0.003*\"beric actual\" + 0.003*\"adding lord\" + 0.002*\"blessing kind\" + 0.002*\"burning chanting\" + 0.002*\"baratheon chaos\" + 0.002*\"accepted watch\" + 0.002*\"brother exactly\" + 0.002*\"brother come\" + 0.002*\"bed opposite\"\n",
      "2019-05-19 16:38:25,965 : INFO : topic #2 (0.143): 0.005*\"arya wandering\" + 0.005*\"believe greenseer\" + 0.004*\"believe leader\" + 0.003*\"bran behaving\" + 0.003*\"blame burning\" + 0.002*\"bran game\" + 0.002*\"army strongest\" + 0.002*\"army stay\" + 0.002*\"abandon belief\" + 0.002*\"birthday huge\"\n",
      "2019-05-19 16:38:25,994 : INFO : topic #3 (0.143): 0.004*\"buried forever\" + 0.004*\"burn building\" + 0.004*\"bran quiet\" + 0.003*\"bad stark\" + 0.003*\"bad pretty\" + 0.003*\"attracted watching\" + 0.003*\"attention jamie\" + 0.003*\"book general\" + 0.003*\"book explained\" + 0.003*\"believe sex\"\n",
      "2019-05-19 16:38:26,022 : INFO : topic #4 (0.143): 0.004*\"beat nightking\" + 0.004*\"beat logic\" + 0.004*\"beat revelation\" + 0.004*\"beloved perfectly\" + 0.003*\"beloved nearly\" + 0.003*\"book realise\" + 0.003*\"arrives fleet\" + 0.003*\"alongside nymeria\" + 0.003*\"battle toying\" + 0.002*\"ally drove\"\n",
      "2019-05-19 16:38:26,048 : INFO : topic #5 (0.143): 0.004*\"arrested daenerys\" + 0.003*\"actor grrm\" + 0.003*\"battle wrapping\" + 0.003*\"able dragonglass\" + 0.003*\"able effect\" + 0.002*\"answer death\" + 0.002*\"answer ask\" + 0.002*\"annoys troop\" + 0.002*\"actual northern\" + 0.002*\"actual night\"\n",
      "2019-05-19 16:38:26,070 : INFO : topic #6 (0.143): 0.004*\"brace worse\" + 0.004*\"arya jorah\" + 0.003*\"bos thing\" + 0.003*\"bay kill\" + 0.002*\"beric overlooked\" + 0.002*\"beric mellisandre\" + 0.002*\"begin snow\" + 0.002*\"begin sprinting\" + 0.002*\"based viewing\" + 0.002*\"bastard level\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"blah foreshadowing\" + 0.004*\"burnt brandon\" + 0.004*\"belong saying\" + 0.004*\"basically attempted\" + 0.004*\"beat make\" + 0.003*\"army sub\" + 0.003*\"additional safety\" + 0.003*\"alot shitting\" + 0.002*\"broken seen\" + 0.002*\"away writing\"'),\n",
       " (1,\n",
       "  '0.004*\"bran tasked\" + 0.003*\"beric actual\" + 0.003*\"adding lord\" + 0.002*\"blessing kind\" + 0.002*\"burning chanting\" + 0.002*\"baratheon chaos\" + 0.002*\"accepted watch\" + 0.002*\"brother exactly\" + 0.002*\"brother come\" + 0.002*\"bed opposite\"'),\n",
       " (2,\n",
       "  '0.005*\"arya wandering\" + 0.005*\"believe greenseer\" + 0.004*\"believe leader\" + 0.003*\"bran behaving\" + 0.003*\"blame burning\" + 0.002*\"bran game\" + 0.002*\"army strongest\" + 0.002*\"army stay\" + 0.002*\"abandon belief\" + 0.002*\"birthday huge\"'),\n",
       " (3,\n",
       "  '0.004*\"buried forever\" + 0.004*\"burn building\" + 0.004*\"bran quiet\" + 0.003*\"bad stark\" + 0.003*\"bad pretty\" + 0.003*\"attracted watching\" + 0.003*\"attention jamie\" + 0.003*\"book general\" + 0.003*\"book explained\" + 0.003*\"believe sex\"'),\n",
       " (4,\n",
       "  '0.004*\"beat nightking\" + 0.004*\"beat logic\" + 0.004*\"beat revelation\" + 0.004*\"beloved perfectly\" + 0.003*\"beloved nearly\" + 0.003*\"book realise\" + 0.003*\"arrives fleet\" + 0.003*\"alongside nymeria\" + 0.003*\"battle toying\" + 0.002*\"ally drove\"'),\n",
       " (5,\n",
       "  '0.004*\"arrested daenerys\" + 0.003*\"actor grrm\" + 0.003*\"battle wrapping\" + 0.003*\"able dragonglass\" + 0.003*\"able effect\" + 0.002*\"answer death\" + 0.002*\"answer ask\" + 0.002*\"annoys troop\" + 0.002*\"actual northern\" + 0.002*\"actual night\"'),\n",
       " (6,\n",
       "  '0.004*\"brace worse\" + 0.004*\"arya jorah\" + 0.003*\"bos thing\" + 0.003*\"bay kill\" + 0.002*\"beric overlooked\" + 0.002*\"beric mellisandre\" + 0.002*\"begin snow\" + 0.002*\"begin sprinting\" + 0.002*\"based viewing\" + 0.002*\"bastard level\"')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_count.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
