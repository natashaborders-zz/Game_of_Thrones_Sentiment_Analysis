{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "got = pickle.load( open( \"./Data/final_got_monday.pkl\", \"rb\" ) )\n",
    "#asoiaf_reddit = pickle.load( open( \"./Data/soiaf_reddit_ready_vector.pkl\", \"rb\" ) )\n",
    "#freefolk_reddit = pickle.load( open( \"./Data/freefolk_reddit_ready_vector.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153179"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(got)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153179, 7231)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer = 'word', encoding = 'string',  strip_accents = 'unicode',\n",
    "                                   stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "count_got = count_vectorizer.fit_transform(got)\n",
    "count_got.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_got_pd = pd.DataFrame(count_got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 448)\\t1\\n  (0, 498)\\t1\\n  (0, 505)\\t1\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 1518)\\t1\\n  (0, 1909)\\t1\\n  (0, 2685)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 684)\\t1\\n  (0, 1455)\\t1\\n  (0, 1567)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 2726)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 7)\\t1\\n  (0, 16)\\t2\\n  (0, 59)\\t1\\n  (0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(0, 684)\\t1\\n  (0, 926)\\t1\\n  (0, 1455)\\t1\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0, 2726)\\t1\\n  (0, 4153)\\t1\\n  (0, 4174)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(0, 317)\\t1\\n  (0, 319)\\t1\\n  (0, 577)\\t1\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(0, 118)\\t1\\n  (0, 362)\\t1\\n  (0, 585)\\t1\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(0, 591)\\t2\\n  (0, 1577)\\t2\\n  (0, 2615)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(0, 7)\\t6\\n  (0, 317)\\t2\\n  (0, 674)\\t2\\n  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(0, 100)\\t1\\n  (0, 199)\\t1\\n  (0, 313)\\t1\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(0, 25)\\t1\\n  (0, 1514)\\t2\\n  (0, 1732)\\t1\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(0, 268)\\t1\\n  (0, 434)\\t1\\n  (0, 477)\\t2\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(0, 7)\\t6\\n  (0, 317)\\t2\\n  (0, 674)\\t2\\n  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(0, 1524)\\t1\\n  (0, 4554)\\t1\\n  (0, 5968)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(0, 59)\\t1\\n  (0, 66)\\t1\\n  (0, 716)\\t2\\n  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(0, 1168)\\t1\\n  (0, 1518)\\t1\\n  (0, 1909)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(0, 59)\\t1\\n  (0, 66)\\t1\\n  (0, 716)\\t2\\n  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(0, 1544)\\t1\\n  (0, 5407)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(0, 7)\\t1\\n  (0, 568)\\t1\\n  (0, 643)\\t4\\n  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(0, 1937)\\t1\\n  (0, 2050)\\t1\\n  (0, 2726)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(0, 4174)\\t1\\n  (0, 4280)\\t1\\n  (0, 4803)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(0, 3701)\\t1\\n  (0, 4560)\\t1\\n  (0, 4987)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(0, 2396)\\t1\\n  (0, 3442)\\t1\\n  (0, 6364)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(0, 827)\\t1\\n  (0, 2523)\\t1\\n  (0, 3084)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(0, 1937)\\t1\\n  (0, 2050)\\t1\\n  (0, 2726)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(0, 1577)\\t1\\n  (0, 1934)\\t1\\n  (0, 2475)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(0, 2483)\\t1\\n  (0, 4174)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(0, 254)\\t1\\n  (0, 331)\\t2\\n  (0, 419)\\t2\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153149</th>\n",
       "      <td>(0, 730)\\t1\\n  (0, 2685)\\t1\\n  (0, 4153)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153150</th>\n",
       "      <td>(0, 6621)\\t1\\n  (0, 7109)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153151</th>\n",
       "      <td>(0, 597)\\t1\\n  (0, 2708)\\t1\\n  (0, 4434)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153152</th>\n",
       "      <td>(0, 2702)\\t1\\n  (0, 6435)\\t1\\n  (0, 7109)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153153</th>\n",
       "      <td>(0, 667)\\t1\\n  (0, 4991)\\t1\\n  (0, 5697)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153154</th>\n",
       "      <td>(0, 6621)\\t2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153155</th>\n",
       "      <td>(0, 139)\\t1\\n  (0, 233)\\t1\\n  (0, 643)\\t1\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153156</th>\n",
       "      <td>(0, 706)\\t1\\n  (0, 2685)\\t1\\n  (0, 2901)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153157</th>\n",
       "      <td>(0, 1919)\\t1\\n  (0, 6471)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153158</th>\n",
       "      <td>(0, 2615)\\t1\\n  (0, 2726)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>(0, 2669)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>(0, 2702)\\t1\\n  (0, 6577)\\t1\\n  (0, 6596)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>(0, 6422)\\t1\\n  (0, 6621)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>(0, 2726)\\t1\\n  (0, 3719)\\t1\\n  (0, 5308)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153164</th>\n",
       "      <td>(0, 444)\\t1\\n  (0, 1302)\\t1\\n  (0, 1544)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153165</th>\n",
       "      <td>(0, 174)\\t1\\n  (0, 477)\\t1\\n  (0, 5306)\\t1\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153166</th>\n",
       "      <td>(0, 1934)\\t1\\n  (0, 3388)\\t1\\n  (0, 3483)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153167</th>\n",
       "      <td>(0, 2669)\\t1\\n  (0, 6471)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153168</th>\n",
       "      <td>(0, 1934)\\t1\\n  (0, 3483)\\t1\\n  (0, 3484)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153169</th>\n",
       "      <td>(0, 444)\\t1\\n  (0, 951)\\t1\\n  (0, 3518)\\t1\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153170</th>\n",
       "      <td>(0, 370)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153171</th>\n",
       "      <td>(0, 3949)\\t1\\n  (0, 7208)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153172</th>\n",
       "      <td>(0, 5005)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153173</th>\n",
       "      <td>(0, 951)\\t1\\n  (0, 1369)\\t1\\n  (0, 1484)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153174</th>\n",
       "      <td>(0, 264)\\t1\\n  (0, 1934)\\t1\\n  (0, 2354)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153175</th>\n",
       "      <td>(0, 2708)\\t1\\n  (0, 2834)\\t1\\n  (0, 3450)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153176</th>\n",
       "      <td>(0, 985)\\t1\\n  (0, 5482)\\t1\\n  (0, 5959)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153177</th>\n",
       "      <td>(0, 1524)\\t1\\n  (0, 1919)\\t1\\n  (0, 2669)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153178</th>\n",
       "      <td>(0, 1081)\\t1\\n  (0, 1934)\\t1\\n  (0, 6398)\\t1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153179 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "0         (0, 448)\\t1\\n  (0, 498)\\t1\\n  (0, 505)\\t1\\n ...\n",
       "1         (0, 1518)\\t1\\n  (0, 1909)\\t1\\n  (0, 2685)\\t1...\n",
       "2         (0, 684)\\t1\\n  (0, 1455)\\t1\\n  (0, 1567)\\t1\\...\n",
       "3                                            (0, 2726)\\t1\n",
       "4         (0, 7)\\t1\\n  (0, 16)\\t2\\n  (0, 59)\\t1\\n  (0,...\n",
       "5         (0, 684)\\t1\\n  (0, 926)\\t1\\n  (0, 1455)\\t1\\n...\n",
       "6         (0, 2726)\\t1\\n  (0, 4153)\\t1\\n  (0, 4174)\\t1...\n",
       "7         (0, 317)\\t1\\n  (0, 319)\\t1\\n  (0, 577)\\t1\\n ...\n",
       "8         (0, 118)\\t1\\n  (0, 362)\\t1\\n  (0, 585)\\t1\\n ...\n",
       "9         (0, 591)\\t2\\n  (0, 1577)\\t2\\n  (0, 2615)\\t1\\...\n",
       "10        (0, 7)\\t6\\n  (0, 317)\\t2\\n  (0, 674)\\t2\\n  (...\n",
       "11        (0, 100)\\t1\\n  (0, 199)\\t1\\n  (0, 313)\\t1\\n ...\n",
       "12        (0, 25)\\t1\\n  (0, 1514)\\t2\\n  (0, 1732)\\t1\\n...\n",
       "13        (0, 268)\\t1\\n  (0, 434)\\t1\\n  (0, 477)\\t2\\n ...\n",
       "14        (0, 7)\\t6\\n  (0, 317)\\t2\\n  (0, 674)\\t2\\n  (...\n",
       "15           (0, 1524)\\t1\\n  (0, 4554)\\t1\\n  (0, 5968)\\t1\n",
       "16        (0, 59)\\t1\\n  (0, 66)\\t1\\n  (0, 716)\\t2\\n  (...\n",
       "17        (0, 1168)\\t1\\n  (0, 1518)\\t1\\n  (0, 1909)\\t1...\n",
       "18        (0, 59)\\t1\\n  (0, 66)\\t1\\n  (0, 716)\\t2\\n  (...\n",
       "19                           (0, 1544)\\t1\\n  (0, 5407)\\t1\n",
       "20        (0, 7)\\t1\\n  (0, 568)\\t1\\n  (0, 643)\\t4\\n  (...\n",
       "21        (0, 1937)\\t1\\n  (0, 2050)\\t1\\n  (0, 2726)\\t1...\n",
       "22        (0, 4174)\\t1\\n  (0, 4280)\\t1\\n  (0, 4803)\\t1...\n",
       "23        (0, 3701)\\t1\\n  (0, 4560)\\t1\\n  (0, 4987)\\t1...\n",
       "24           (0, 2396)\\t1\\n  (0, 3442)\\t1\\n  (0, 6364)\\t1\n",
       "25        (0, 827)\\t1\\n  (0, 2523)\\t1\\n  (0, 3084)\\t1\\...\n",
       "26        (0, 1937)\\t1\\n  (0, 2050)\\t1\\n  (0, 2726)\\t1...\n",
       "27        (0, 1577)\\t1\\n  (0, 1934)\\t1\\n  (0, 2475)\\t1...\n",
       "28                           (0, 2483)\\t1\\n  (0, 4174)\\t1\n",
       "29        (0, 254)\\t1\\n  (0, 331)\\t2\\n  (0, 419)\\t2\\n ...\n",
       "...                                                   ...\n",
       "153149    (0, 730)\\t1\\n  (0, 2685)\\t1\\n  (0, 4153)\\t1\\...\n",
       "153150                       (0, 6621)\\t1\\n  (0, 7109)\\t1\n",
       "153151    (0, 597)\\t1\\n  (0, 2708)\\t1\\n  (0, 4434)\\t1\\...\n",
       "153152       (0, 2702)\\t1\\n  (0, 6435)\\t1\\n  (0, 7109)\\t1\n",
       "153153    (0, 667)\\t1\\n  (0, 4991)\\t1\\n  (0, 5697)\\t1\\...\n",
       "153154                                       (0, 6621)\\t2\n",
       "153155    (0, 139)\\t1\\n  (0, 233)\\t1\\n  (0, 643)\\t1\\n ...\n",
       "153156    (0, 706)\\t1\\n  (0, 2685)\\t1\\n  (0, 2901)\\t1\\...\n",
       "153157                       (0, 1919)\\t1\\n  (0, 6471)\\t1\n",
       "153158                       (0, 2615)\\t1\\n  (0, 2726)\\t1\n",
       "153159                                       (0, 2669)\\t1\n",
       "153160    (0, 2702)\\t1\\n  (0, 6577)\\t1\\n  (0, 6596)\\t1...\n",
       "153161                                                   \n",
       "153162                       (0, 6422)\\t1\\n  (0, 6621)\\t1\n",
       "153163    (0, 2726)\\t1\\n  (0, 3719)\\t1\\n  (0, 5308)\\t1...\n",
       "153164    (0, 444)\\t1\\n  (0, 1302)\\t1\\n  (0, 1544)\\t1\\...\n",
       "153165    (0, 174)\\t1\\n  (0, 477)\\t1\\n  (0, 5306)\\t1\\n...\n",
       "153166       (0, 1934)\\t1\\n  (0, 3388)\\t1\\n  (0, 3483)\\t1\n",
       "153167                       (0, 2669)\\t1\\n  (0, 6471)\\t1\n",
       "153168    (0, 1934)\\t1\\n  (0, 3483)\\t1\\n  (0, 3484)\\t1...\n",
       "153169    (0, 444)\\t1\\n  (0, 951)\\t1\\n  (0, 3518)\\t1\\n...\n",
       "153170                                        (0, 370)\\t1\n",
       "153171                       (0, 3949)\\t1\\n  (0, 7208)\\t1\n",
       "153172                                       (0, 5005)\\t1\n",
       "153173    (0, 951)\\t1\\n  (0, 1369)\\t1\\n  (0, 1484)\\t1\\...\n",
       "153174    (0, 264)\\t1\\n  (0, 1934)\\t1\\n  (0, 2354)\\t1\\...\n",
       "153175    (0, 2708)\\t1\\n  (0, 2834)\\t1\\n  (0, 3450)\\t1...\n",
       "153176        (0, 985)\\t1\\n  (0, 5482)\\t1\\n  (0, 5959)\\t1\n",
       "153177    (0, 1524)\\t1\\n  (0, 1919)\\t1\\n  (0, 2669)\\t1...\n",
       "153178       (0, 1081)\\t1\\n  (0, 1934)\\t1\\n  (0, 6398)\\t1\n",
       "\n",
       "[153179 rows x 1 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA/PCA/SVD ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(153179, 7231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17006132, 0.10962433, 0.10199264])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "#  Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "lsa_count = TruncatedSVD(3)\n",
    "count_doc_topic = lsa_count.fit_transform(count_got)\n",
    "lsa_count.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abdicate</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>ability</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abolish</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yunkai</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 7231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             abandon  abandoned  abandonment  abdicate  abdomen  ability  \\\n",
       "component_1    0.000      0.000          0.0       0.0      0.0    0.001   \n",
       "component_2    0.002      0.002          0.0       0.0      0.0    0.007   \n",
       "component_3   -0.000     -0.000         -0.0      -0.0     -0.0   -0.001   \n",
       "\n",
       "             ablaze   able  aboard  abolish  ...  youtube  yunkai  yup   zero  \\\n",
       "component_1   0.000  0.002     0.0      0.0  ...      0.0   0.000  0.0  0.000   \n",
       "component_2   0.001  0.025     0.0      0.0  ...      0.0   0.002  0.0  0.003   \n",
       "component_3  -0.000 -0.002    -0.0     -0.0  ...     -0.0  -0.000 -0.0 -0.000   \n",
       "\n",
       "             zig  zimmer  zombie  zone  zoned  zoom  \n",
       "component_1  0.0     0.0   0.000   0.0    0.0   0.0  \n",
       "component_2  0.0     0.0   0.004   0.0    0.0   0.0  \n",
       "component_3 -0.0     0.0  -0.000  -0.0   -0.0  -0.0  \n",
       "\n",
       "[3 rows x 7231 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "got_count_topic_word = pd.DataFrame(lsa_count.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", 'component_3'],\n",
    "             columns = count_vectorizer.get_feature_names())\n",
    "got_count_topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "plot, armor, jon, night, dany, bran, like, arya, cersei, people, think, battle, time, way, throne\n",
      "\n",
      "Topic  1\n",
      "jon, night, dany, bran, arya, like, cersei, think, people, battle, time, throne, dead, dragon, way\n",
      "\n",
      "Topic  2\n",
      "hold, door, armor, plot, hodor, tumblin, wylis, beer, vi, bus, stargaryen, rope, oak, furdik, reconnaissance\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_count, count_vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abdicate</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>ability</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abolish</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yunkai</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>watch wait curious see watch c...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picked dragon glass dagger fou...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book question man without face...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enough time binge list must wa...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book question faceless man old...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need subscription watch new go...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watching six wonder time robin...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samwell tarly daenerys sure so...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got death bingo made death bin...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 7231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   abandon  abandoned  abandonment  abdicate  \\\n",
       "watch wait curious see watch c...        0          0            0         0   \n",
       "picked dragon glass dagger fou...        0          0            0         0   \n",
       "book question man without face...        0          0            0         0   \n",
       "got...                                   0          0            0         0   \n",
       "enough time binge list must wa...        0          0            0         0   \n",
       "book question faceless man old...        0          0            0         0   \n",
       "need subscription watch new go...        0          0            0         0   \n",
       "watching six wonder time robin...        0          0            0         0   \n",
       "samwell tarly daenerys sure so...        0          0            0         0   \n",
       "got death bingo made death bin...        0          0            0         0   \n",
       "\n",
       "                                   abdomen  ability  ablaze  able  aboard  \\\n",
       "watch wait curious see watch c...        0        0       0     0       0   \n",
       "picked dragon glass dagger fou...        0        0       0     0       0   \n",
       "book question man without face...        0        0       0     0       0   \n",
       "got...                                   0        0       0     0       0   \n",
       "enough time binge list must wa...        0        0       0     1       0   \n",
       "book question faceless man old...        0        0       0     0       0   \n",
       "need subscription watch new go...        0        0       0     0       0   \n",
       "watching six wonder time robin...        0        0       0     0       0   \n",
       "samwell tarly daenerys sure so...        0        0       0     0       0   \n",
       "got death bingo made death bin...        0        0       0     0       0   \n",
       "\n",
       "                                   abolish  ...  youtube  yunkai  yup  zero  \\\n",
       "watch wait curious see watch c...        0  ...        0       0    0     0   \n",
       "picked dragon glass dagger fou...        0  ...        0       0    0     0   \n",
       "book question man without face...        0  ...        0       0    0     0   \n",
       "got...                                   0  ...        0       0    0     0   \n",
       "enough time binge list must wa...        0  ...        0       1    0     1   \n",
       "book question faceless man old...        0  ...        0       0    0     0   \n",
       "need subscription watch new go...        0  ...        0       0    0     0   \n",
       "watching six wonder time robin...        0  ...        0       0    0     0   \n",
       "samwell tarly daenerys sure so...        0  ...        0       0    0     0   \n",
       "got death bingo made death bin...        0  ...        0       0    0     0   \n",
       "\n",
       "                                   zig  zimmer  zombie  zone  zoned  zoom  \n",
       "watch wait curious see watch c...    0       0       0     0      0     0  \n",
       "picked dragon glass dagger fou...    0       0       0     0      0     0  \n",
       "book question man without face...    0       0       0     0      0     0  \n",
       "got...                               0       0       0     0      0     0  \n",
       "enough time binge list must wa...    0       0       0     0      0     0  \n",
       "book question faceless man old...    0       0       0     0      0     0  \n",
       "need subscription watch new go...    0       0       0     0      0     0  \n",
       "watching six wonder time robin...    0       0       0     0      0     0  \n",
       "samwell tarly daenerys sure so...    0       0       0     0      0     0  \n",
       "got death bingo made death bin...    0       0       0     0      0     0  \n",
       "\n",
       "[10 rows x 7231 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "got_label = [e[:30]+\"...\" for e in got]\n",
    "pd.DataFrame(count_got.toarray(), index=got_label, columns=count_vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nmf_model = NMF(3)\n",
    "count_nmf_topic = count_nmf_model.fit_transform(count_got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_topic_word_got = pd.DataFrame(count_nmf_model.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", 'component_3'],\n",
    "             columns = count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "plot, armor, like, story, got, battle, writing, series, people, main, way, really, game, time, point\n",
      "\n",
      "Topic  1\n",
      "jon, night, dany, bran, arya, like, cersei, think, people, battle, time, throne, dead, dragon, way\n",
      "\n",
      "Topic  2\n",
      "hold, door, hodor, bran, dead, time, past, jaime, dothraki, undead, castle, wildfire, meera, unsullied, winterfell\n"
     ]
    }
   ],
   "source": [
    "display_topics(count_nmf_model, count_vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = pd.DataFrame(count_doc_topic.round(10),\n",
    "             index = got_label,\n",
    "             columns = [\"component_1\",\"component_2\", 'component_3', 'component_4', 'component_5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(count_got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7231"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-20 16:48:16,151 : INFO : using symmetric alpha at 0.06666666666666667\n",
      "2019-05-20 16:48:16,155 : INFO : using symmetric eta at 0.06666666666666667\n",
      "2019-05-20 16:48:16,158 : INFO : using serial LDA version on this node\n",
      "2019-05-20 16:48:16,171 : INFO : running online (multi-pass) LDA training, 15 topics, 5 passes over the supplied corpus of 7231 documents, updating model once every 2000 documents, evaluating perplexity every 7231 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-05-20 16:48:16,335 : INFO : PROGRESS: pass 0, at document #2000/7231\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 10048 is out of bounds for axis 1 with size 7231",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-752503f23e34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create lda model (equivalent to \"fit\" in sklearn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlda_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    718\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                     )\n\u001b[0;32m--> 720\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElogtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpElogtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0mexpElogbetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10048 is out of bounds for axis 1 with size 7231"
     ]
    }
   ],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda_count = models.LdaModel(corpus=corpus, num_topics=15, id2word=id2word, passes=5) # train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-20 12:31:30,953 : INFO : topic #0 (0.200): 0.003*\"assume sex\" + 0.002*\"cersei shelter\" + 0.002*\"arya chase\" + 0.002*\"certainly justify\" + 0.002*\"bell audience\" + 0.002*\"bell favorite\" + 0.002*\"bell concentrated\" + 0.002*\"bell braid\" + 0.002*\"briene south\" + 0.002*\"car dothraki\"\n",
      "2019-05-20 12:31:30,966 : INFO : topic #1 (0.200): 0.003*\"assassin darth\" + 0.003*\"better choice\" + 0.002*\"bring martha\" + 0.002*\"added related\" + 0.002*\"body burnt\" + 0.002*\"burning conquering\" + 0.002*\"cersei board\" + 0.001*\"believe happening\" + 0.001*\"bulk wight\" + 0.001*\"based individual\"\n",
      "2019-05-20 12:31:30,978 : INFO : topic #2 (0.200): 0.003*\"cersei tragically\" + 0.003*\"cersei layer\" + 0.003*\"big fleet\" + 0.003*\"big filler\" + 0.002*\"bran pussy\" + 0.002*\"cersei hollywood\" + 0.002*\"bigger course\" + 0.002*\"addition happy\" + 0.002*\"alternative like\" + 0.002*\"believe ala\"\n",
      "2019-05-20 12:31:30,989 : INFO : topic #3 (0.200): 0.003*\"better analysis\" + 0.002*\"actor cool\" + 0.002*\"broad secret\" + 0.002*\"change completely\" + 0.002*\"bean watch\" + 0.002*\"charisma make\" + 0.002*\"care forgetting\" + 0.001*\"absolute nose\" + 0.001*\"absolute snack\" + 0.001*\"absence\"\n",
      "2019-05-20 12:31:30,999 : INFO : topic #4 (0.200): 0.003*\"body able\" + 0.003*\"big boost\" + 0.002*\"candidate rule\" + 0.002*\"canon end\" + 0.002*\"arya left\" + 0.002*\"bran ironborn\" + 0.002*\"bran ice\" + 0.002*\"abandon arc\" + 0.001*\"ash world\" + 0.001*\"causing unite\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"assume sex\" + 0.002*\"cersei shelter\" + 0.002*\"arya chase\" + 0.002*\"certainly justify\" + 0.002*\"bell audience\" + 0.002*\"bell favorite\" + 0.002*\"bell concentrated\" + 0.002*\"bell braid\" + 0.002*\"briene south\" + 0.002*\"car dothraki\"'),\n",
       " (1,\n",
       "  '0.003*\"assassin darth\" + 0.003*\"better choice\" + 0.002*\"bring martha\" + 0.002*\"added related\" + 0.002*\"body burnt\" + 0.002*\"burning conquering\" + 0.002*\"cersei board\" + 0.001*\"believe happening\" + 0.001*\"bulk wight\" + 0.001*\"based individual\"'),\n",
       " (2,\n",
       "  '0.003*\"cersei tragically\" + 0.003*\"cersei layer\" + 0.003*\"big fleet\" + 0.003*\"big filler\" + 0.002*\"bran pussy\" + 0.002*\"cersei hollywood\" + 0.002*\"bigger course\" + 0.002*\"addition happy\" + 0.002*\"alternative like\" + 0.002*\"believe ala\"'),\n",
       " (3,\n",
       "  '0.003*\"better analysis\" + 0.002*\"actor cool\" + 0.002*\"broad secret\" + 0.002*\"change completely\" + 0.002*\"bean watch\" + 0.002*\"charisma make\" + 0.002*\"care forgetting\" + 0.001*\"absolute nose\" + 0.001*\"absolute snack\" + 0.001*\"absence\"'),\n",
       " (4,\n",
       "  '0.003*\"body able\" + 0.003*\"big boost\" + 0.002*\"candidate rule\" + 0.002*\"canon end\" + 0.002*\"arya left\" + 0.002*\"bran ironborn\" + 0.002*\"bran ice\" + 0.002*\"abandon arc\" + 0.001*\"ash world\" + 0.001*\"causing unite\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_count.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-19 16:07:39,640 : INFO : saving LdaState object under /anaconda3/lib/python3.7/site-packages/gensim/test/test_data/model.state, separately None\n",
      "2019-05-19 16:07:39,662 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-19 16:07:40,258 : INFO : saved /anaconda3/lib/python3.7/site-packages/gensim/test/test_data/model.state\n",
      "2019-05-19 16:07:40,267 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-19 16:07:48,247 : INFO : saving LdaModel object under /anaconda3/lib/python3.7/site-packages/gensim/test/test_data/model, separately ['expElogbeta', 'sstats']\n",
      "2019-05-19 16:07:48,262 : INFO : storing np array 'expElogbeta' to /anaconda3/lib/python3.7/site-packages/gensim/test/test_data/model.expElogbeta.npy\n",
      "2019-05-19 16:07:48,477 : INFO : not storing attribute dispatcher\n",
      "2019-05-19 16:07:48,478 : INFO : not storing attribute state\n",
      "2019-05-19 16:07:48,479 : INFO : not storing attribute id2word\n",
      "2019-05-19 16:07:48,481 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-19 16:07:48,507 : INFO : saved /anaconda3/lib/python3.7/site-packages/gensim/test/test_data/model\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Save model to disk.\n",
    "lda_GOT = datapath(\"model\")\n",
    "lda.save(lda_GOT)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "#lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda = LdaModel(corpus, num_topics=100)  # train model\n",
    "#print(lda[doc_bow]) # get topic probability distribution for a document\n",
    "#lda.update(corpus2) # update the LDA model with additional documents\n",
    "#print(lda[doc_bow])\n",
    "\n",
    "#lda = LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)  # train asymmetric alpha from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF Vectorizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sklearn, it's VERY similar to how we did CountVectorizer\n",
    "tf_idf_vectorizer = TfidfVectorizer(  \n",
    "                                   stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "got_tfidf = tf_idf_vectorizer.fit_transform(got)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA/PCA/SVD ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00799442, 0.01226075])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "#  Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "tf_idf_lsa = TruncatedSVD(2)\n",
    "tf_idf_doc_topic = tf_idf_lsa.fit_transform(got_tfidf)\n",
    "tf_idf_lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_lsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abdicate</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>ability</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abolish</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yunkai</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 7231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             abandon  abandoned  abandonment  abdicate  abdomen  ability  \\\n",
       "component_1      0.0        0.0          0.0       0.0      0.0     0.01   \n",
       "component_2     -0.0       -0.0         -0.0       0.0     -0.0    -0.00   \n",
       "\n",
       "             ablaze  able  aboard  abolish  ...  youtube  yunkai  yup  zero  \\\n",
       "component_1     0.0  0.02     0.0      0.0  ...      0.0     0.0  0.0   0.0   \n",
       "component_2    -0.0 -0.01    -0.0      0.0  ...      0.0    -0.0 -0.0  -0.0   \n",
       "\n",
       "             zig  zimmer  zombie  zone  zoned  zoom  \n",
       "component_1  0.0     0.0    0.01   0.0    0.0   0.0  \n",
       "component_2 -0.0    -0.0   -0.00   0.0   -0.0  -0.0  \n",
       "\n",
       "[2 rows x 7231 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_topic_word = pd.DataFrame(tf_idf_lsa.components_.round(2),\n",
    "             index = [\"component_1\",\"component_2\"],\n",
    "             columns = tf_idf_vectorizer.get_feature_names())\n",
    "tf_idf_topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran = tf_idf_topic_word.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "game          0.68\n",
       "throne        0.59\n",
       "iron          0.05\n",
       "watch         0.02\n",
       "theme         0.02\n",
       "petition      0.01\n",
       "premiere      0.01\n",
       "cover         0.01\n",
       "style         0.01\n",
       "sit           0.01\n",
       "song          0.01\n",
       "edition       0.01\n",
       "fan           0.01\n",
       "official      0.01\n",
       "pool          0.01\n",
       "inspired      0.01\n",
       "remake        0.01\n",
       "trailer       0.01\n",
       "video         0.01\n",
       "review        0.01\n",
       "promo         0.01\n",
       "win           0.01\n",
       "finale        0.01\n",
       "recap         0.01\n",
       "fireproof    -0.00\n",
       "fleshed      -0.00\n",
       "flawed       -0.00\n",
       "flawless     -0.00\n",
       "flea         -0.00\n",
       "fleabottom    0.00\n",
       "              ... \n",
       "sansa        -0.03\n",
       "landing      -0.03\n",
       "got          -0.03\n",
       "jaime        -0.03\n",
       "maybe        -0.03\n",
       "thought      -0.03\n",
       "snow         -0.03\n",
       "way          -0.03\n",
       "stark        -0.03\n",
       "theory       -0.03\n",
       "time         -0.04\n",
       "dragon       -0.04\n",
       "white        -0.04\n",
       "tyrion       -0.04\n",
       "people       -0.04\n",
       "really       -0.04\n",
       "going        -0.04\n",
       "dead         -0.04\n",
       "winterfell   -0.04\n",
       "know         -0.04\n",
       "battle       -0.05\n",
       "kill         -0.05\n",
       "cersei       -0.06\n",
       "think        -0.07\n",
       "like         -0.08\n",
       "dany         -0.09\n",
       "arya         -0.12\n",
       "jon          -0.13\n",
       "night        -0.18\n",
       "bran         -0.19\n",
       "Name: component_2, Length: 7231, dtype: float64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran['component_2'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "throne, night, game, jon, bran, like, arya, dany, think, got, cersei, people, know, end, battle\n",
      "\n",
      "Topic  1\n",
      "game, throne, iron, watch, theme, review, recap, sit, video, premiere, cover, remake, finale, petition, win\n"
     ]
    }
   ],
   "source": [
    "display_topics(tf_idf_lsa, tf_idf_vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### NMF ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_nmf_model = NMF(2)\n",
    "nmf_doc_topic = tf_idf_nmf_model.fit_transform(got_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_topic_word_got = pd.DataFrame(tf_idf_nmf_model.components_.round(2),\n",
    "             index = [\"component_1\",\"component_2\"],\n",
    "             columns = tf_idf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "night, jon, bran, arya, like, dany, think, got, cersei, people, know, battle, time, kill, theory, end, dragon, really, going, tyrion, dead, daenerys, way, winterfell, scene, snow, sansa, white, die, thought, good, jaime, army, death, landing, make, stark, long, ending, queen\n",
      "\n",
      "Topic  1\n",
      "game, throne, iron, watch, end, ending, theme, final, series, finale, new, watching, win, sit, video, fan, recap, review, best, got, theory, premiere, song, prediction, cover, remake, petition, trailer, tonight, day, pool, want, daenerys, music, play, live, claim, real, week, favorite\n"
     ]
    }
   ],
   "source": [
    "display_topics(tf_idf_nmf_model, tf_idf_vectorizer.get_feature_names(), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(got_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in tf_idf_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-20 16:50:23,410 : INFO : using symmetric alpha at 0.5\n",
      "2019-05-20 16:50:23,411 : INFO : using symmetric eta at 0.5\n",
      "2019-05-20 16:50:23,413 : INFO : using serial LDA version on this node\n",
      "2019-05-20 16:50:23,417 : INFO : running online (single-pass) LDA training, 2 topics, 1 passes over the supplied corpus of 7231 documents, updating model once every 2000 documents, evaluating perplexity every 7231 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-05-20 16:50:23,418 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2019-05-20 16:50:23,557 : INFO : PROGRESS: pass 0, at document #2000/7231\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 10048 is out of bounds for axis 1 with size 7231",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-c83a32602b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create lda model (equivalent to \"fit\" in sklearn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlda_tf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    718\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                     )\n\u001b[0;32m--> 720\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElogtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpElogtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0mexpElogbetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10048 is out of bounds for axis 1 with size 7231"
     ]
    }
   ],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda_tf_idf = models.LdaModel(corpus=corpus, num_topics=2, id2word=id2word, passes=1) # train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-19 16:38:25,914 : INFO : topic #0 (0.143): 0.005*\"blah foreshadowing\" + 0.004*\"burnt brandon\" + 0.004*\"belong saying\" + 0.004*\"basically attempted\" + 0.004*\"beat make\" + 0.003*\"army sub\" + 0.003*\"additional safety\" + 0.003*\"alot shitting\" + 0.002*\"broken seen\" + 0.002*\"away writing\"\n",
      "2019-05-19 16:38:25,939 : INFO : topic #1 (0.143): 0.004*\"bran tasked\" + 0.003*\"beric actual\" + 0.003*\"adding lord\" + 0.002*\"blessing kind\" + 0.002*\"burning chanting\" + 0.002*\"baratheon chaos\" + 0.002*\"accepted watch\" + 0.002*\"brother exactly\" + 0.002*\"brother come\" + 0.002*\"bed opposite\"\n",
      "2019-05-19 16:38:25,965 : INFO : topic #2 (0.143): 0.005*\"arya wandering\" + 0.005*\"believe greenseer\" + 0.004*\"believe leader\" + 0.003*\"bran behaving\" + 0.003*\"blame burning\" + 0.002*\"bran game\" + 0.002*\"army strongest\" + 0.002*\"army stay\" + 0.002*\"abandon belief\" + 0.002*\"birthday huge\"\n",
      "2019-05-19 16:38:25,994 : INFO : topic #3 (0.143): 0.004*\"buried forever\" + 0.004*\"burn building\" + 0.004*\"bran quiet\" + 0.003*\"bad stark\" + 0.003*\"bad pretty\" + 0.003*\"attracted watching\" + 0.003*\"attention jamie\" + 0.003*\"book general\" + 0.003*\"book explained\" + 0.003*\"believe sex\"\n",
      "2019-05-19 16:38:26,022 : INFO : topic #4 (0.143): 0.004*\"beat nightking\" + 0.004*\"beat logic\" + 0.004*\"beat revelation\" + 0.004*\"beloved perfectly\" + 0.003*\"beloved nearly\" + 0.003*\"book realise\" + 0.003*\"arrives fleet\" + 0.003*\"alongside nymeria\" + 0.003*\"battle toying\" + 0.002*\"ally drove\"\n",
      "2019-05-19 16:38:26,048 : INFO : topic #5 (0.143): 0.004*\"arrested daenerys\" + 0.003*\"actor grrm\" + 0.003*\"battle wrapping\" + 0.003*\"able dragonglass\" + 0.003*\"able effect\" + 0.002*\"answer death\" + 0.002*\"answer ask\" + 0.002*\"annoys troop\" + 0.002*\"actual northern\" + 0.002*\"actual night\"\n",
      "2019-05-19 16:38:26,070 : INFO : topic #6 (0.143): 0.004*\"brace worse\" + 0.004*\"arya jorah\" + 0.003*\"bos thing\" + 0.003*\"bay kill\" + 0.002*\"beric overlooked\" + 0.002*\"beric mellisandre\" + 0.002*\"begin snow\" + 0.002*\"begin sprinting\" + 0.002*\"based viewing\" + 0.002*\"bastard level\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"blah foreshadowing\" + 0.004*\"burnt brandon\" + 0.004*\"belong saying\" + 0.004*\"basically attempted\" + 0.004*\"beat make\" + 0.003*\"army sub\" + 0.003*\"additional safety\" + 0.003*\"alot shitting\" + 0.002*\"broken seen\" + 0.002*\"away writing\"'),\n",
       " (1,\n",
       "  '0.004*\"bran tasked\" + 0.003*\"beric actual\" + 0.003*\"adding lord\" + 0.002*\"blessing kind\" + 0.002*\"burning chanting\" + 0.002*\"baratheon chaos\" + 0.002*\"accepted watch\" + 0.002*\"brother exactly\" + 0.002*\"brother come\" + 0.002*\"bed opposite\"'),\n",
       " (2,\n",
       "  '0.005*\"arya wandering\" + 0.005*\"believe greenseer\" + 0.004*\"believe leader\" + 0.003*\"bran behaving\" + 0.003*\"blame burning\" + 0.002*\"bran game\" + 0.002*\"army strongest\" + 0.002*\"army stay\" + 0.002*\"abandon belief\" + 0.002*\"birthday huge\"'),\n",
       " (3,\n",
       "  '0.004*\"buried forever\" + 0.004*\"burn building\" + 0.004*\"bran quiet\" + 0.003*\"bad stark\" + 0.003*\"bad pretty\" + 0.003*\"attracted watching\" + 0.003*\"attention jamie\" + 0.003*\"book general\" + 0.003*\"book explained\" + 0.003*\"believe sex\"'),\n",
       " (4,\n",
       "  '0.004*\"beat nightking\" + 0.004*\"beat logic\" + 0.004*\"beat revelation\" + 0.004*\"beloved perfectly\" + 0.003*\"beloved nearly\" + 0.003*\"book realise\" + 0.003*\"arrives fleet\" + 0.003*\"alongside nymeria\" + 0.003*\"battle toying\" + 0.002*\"ally drove\"'),\n",
       " (5,\n",
       "  '0.004*\"arrested daenerys\" + 0.003*\"actor grrm\" + 0.003*\"battle wrapping\" + 0.003*\"able dragonglass\" + 0.003*\"able effect\" + 0.002*\"answer death\" + 0.002*\"answer ask\" + 0.002*\"annoys troop\" + 0.002*\"actual northern\" + 0.002*\"actual night\"'),\n",
       " (6,\n",
       "  '0.004*\"brace worse\" + 0.004*\"arya jorah\" + 0.003*\"bos thing\" + 0.003*\"bay kill\" + 0.002*\"beric overlooked\" + 0.002*\"beric mellisandre\" + 0.002*\"begin snow\" + 0.002*\"begin sprinting\" + 0.002*\"based viewing\" + 0.002*\"bastard level\"')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_count.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-19 16:39:59,727 : INFO : saving LdaState object under /anaconda3/lib/python3.7/site-packages/gensim/test/test_data/model.state, separately None\n",
      "2019-05-19 16:39:59,733 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-19 16:40:00,235 : INFO : saved /anaconda3/lib/python3.7/site-packages/gensim/test/test_data/model.state\n",
      "2019-05-19 16:40:00,236 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-19 16:40:09,605 : INFO : saving LdaModel object under /anaconda3/lib/python3.7/site-packages/gensim/test/test_data/model, separately ['expElogbeta', 'sstats']\n",
      "2019-05-19 16:40:09,620 : INFO : storing np array 'expElogbeta' to /anaconda3/lib/python3.7/site-packages/gensim/test/test_data/model.expElogbeta.npy\n",
      "2019-05-19 16:40:09,928 : INFO : not storing attribute dispatcher\n",
      "2019-05-19 16:40:09,931 : INFO : not storing attribute state\n",
      "2019-05-19 16:40:09,932 : INFO : not storing attribute id2word\n",
      "2019-05-19 16:40:09,934 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-19 16:40:09,966 : INFO : saved /anaconda3/lib/python3.7/site-packages/gensim/test/test_data/model\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Save model to disk.\n",
    "lda_GOT_tf_idf = datapath(\"model\")\n",
    "lda.save(lda_GOT_tf_idf)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "#lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-4fddbde9477d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtran\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilhouette_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtran\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mlabel_freqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mcheck_number_of_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metric'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/unsupervised.py\u001b[0m in \u001b[0;36mcheck_number_of_labels\u001b[0;34m(n_labels, n_samples)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_labels\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         raise ValueError(\"Number of labels is %d. Valid values are 2 \"\n\u001b[0;32m---> 35\u001b[0;31m                          \"to n_samples - 1 (inclusive)\" % n_labels)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=2, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=opts.verbose)\n",
    "\n",
    "\n",
    "\n",
    "km.fit(tran)\n",
    "\n",
    "print(metrics.silhouette_score(tran, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.18975069e-02, -1.91135734e-03],\n",
       "       [ 2.85000000e-01,  6.35000000e-01],\n",
       "       [-2.16840434e-16,  4.70588235e-06],\n",
       "       [ 4.78861789e-02, -1.65853659e-02],\n",
       "       [ 2.00000000e-01, -1.05555556e-01]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: abandon abandoned\n",
      "Cluster 1: abandoned abandon\n",
      "Cluster 2: abandoned abandon\n",
      "Cluster 3: abandon abandoned\n",
      "Cluster 4: abandon abandoned\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tf_idf_vectorizer.get_feature_names()\n",
    "for i in range(5):\n",
    "    top_ten_words = [terms[ind] for ind in order_centroids[i, :20]]\n",
    "    print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "corpus = [tweet['cleaned_text'] for tweet in tweet_dict_list]\n",
    "count_vectorizer.fit(corpus)\n",
    "counts = count_vectorizer.transform(corpus).transpose()\n",
    "counts.shape\n",
    "corpus = matutils.Sparse2Corpus(counts)\n",
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())\n",
    "#from gensim import models\n",
    "lda2 = models.LdaModel(corpus=corpus, num_topics=5, minimum_probability=0.01, id2word=id2word, passes=10)\n",
    "lda2.print_topics()\n",
    "topic_spread = list(lda2.get_document_topics(corpus))\n",
    "container = []\n",
    "for doc in topic_spread:\n",
    "    tspread = []\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            tspread.append(doc[i][1])\n",
    "        except:\n",
    "            tspread.append(0)\n",
    "    container.append(tspread)\n",
    "pd.DataFrame(container)\n",
    "Collapse\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each episode after LDA.\n",
    "#t-sne plot - look up where words are clustering \n",
    "# do vectorzing by episode and see what clusters of topics and maybe take out the character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
