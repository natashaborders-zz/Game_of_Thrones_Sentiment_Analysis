{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nborders/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nborders/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "got_stop = ['[spoilers]', '[deleted]', 'nan', '[removed]', '[spoiler]', 'deleted', '[Spoilers', 'Main]', 'Spoilers', 'Main', 'removed', 'spoiler', '[All', 'Spoilers]', 'all', 'spoilers', '[NO SPOILER]', 'no spoiler', '[SPOLER]', '(Spoilers', 'Main)', 'spoler', '[Spoilers]', '[SPOILERS]', 'SPOILERS', 'MAIN]', 'MAIN', 'extended)', 'extended', 'Production)', 'Production', 'EXTENDED]', 'EXTENDED', '[NO', 'SPOILERS]', 'NO', '*Spoilers', 'Ahead*', 'Ahead', 'Extended]', 'Extended', '{SPOILER}', 'SPOILER', '[No', 'Spoiler]', 'No', 'Spoiler', '(NO', 'SPOILERS)', 'MAIN)', '(SPOILERS', '(no', 'spoilers)', '[SPOILERS', 'Extended)']\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_reddit = pickle.load( open( \"./Data/got_reddit.pkl\", \"rb\" ) )\n",
    "asoiaf_reddit = pickle.load( open( \"./Data/asoiaf_reddit.pkl\", \"rb\" ) )\n",
    "freefolk_reddit = pickle.load( open( \"./Data/freefolk_reddit.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_punctuation(comment):\n",
    "    punct_list = [i for i in string.punctuation]\n",
    "    \n",
    "    while re.findall(r'\\\\n', comment) != []:\n",
    "        comment = comment.replace(re.search(r'\\\\n', comment)[0], '')\n",
    "    while re.findall(r'\\\\t', comment) != []:\n",
    "        comment = comment.replace(re.search(r'\\\\t', comment)[0], '')\n",
    "    while re.findall(r'\\\\u.{4}', comment) != []:\n",
    "        comment = comment.replace(re.search(r'\\\\u.{4}', comment)[0], '')\n",
    "    \n",
    "    for i in punct_list:\n",
    "        comment = comment.replace(i, ' ')\n",
    "    \n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(comment):\n",
    "    punct_list = [i for i in string.punctuation]\n",
    "    for i in punct_list:\n",
    "        comment = comment.replace(i, ' ')\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_and_lower(comment):\n",
    "    arr = comment.split()\n",
    "    comment = \" \".join(arr).lower()\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove_nums(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(comment):\n",
    "    stop_free = [i for i in comment.split() if i not in stop]\n",
    "    got_stop_free = \" \".join([i for i in stop_free if i not in got_stop])\n",
    "    return got_stop_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(comment):\n",
    "    return \" \".join(lemma.lemmatize(word) for word in comment.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(comment):\n",
    "    return ''.join([char for char in comment if not char.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "got_reddit['clean'] = got_reddit['body'].apply(remove_all_punctuation)\n",
    "got_reddit['clean'] = got_reddit['clean'].apply(trim_and_lower)\n",
    "got_reddit['clean'] = got_reddit['clean'].apply(remove_num)\n",
    "got_reddit['clean'] = got_reddit['clean'].apply(remove_stops)\n",
    "got_reddit['clean'] = got_reddit['clean'].apply(lemmatize)\n",
    "\n",
    "asoiaf_reddit['clean'] = asoiaf_reddit['body'].apply(remove_all_punctuation)\n",
    "asoiaf_reddit['clean'] = asoiaf_reddit['clean'].apply(trim_and_lower)\n",
    "asoiaf_reddit['clean'] = asoiaf_reddit['clean'].apply(remove_num)\n",
    "asoiaf_reddit['clean'] = asoiaf_reddit['clean'].apply(remove_stops)\n",
    "asoiaf_reddit['clean'] = asoiaf_reddit['clean'].apply(lemmatize)\n",
    "\n",
    "freefolk_reddit['clean'] = freefolk_reddit['body'].apply(remove_all_punctuation)\n",
    "freefolk_reddit['clean'] = freefolk_reddit['clean'].apply(trim_and_lower)\n",
    "freefolk_reddit['clean'] = freefolk_reddit['clean'].apply(remove_num)\n",
    "freefolk_reddit['clean'] = freefolk_reddit['clean'].apply(remove_stops)\n",
    "freefolk_reddit['clean'] = freefolk_reddit['clean'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(freefolk_reddit, open(\"freefolk_reddit_ready_vector.pkl\", \"wb\" ) )\n",
    "#pickle.dump(asoiaf_reddit, open(\"soiaf_reddit_ready_vector.pkl\", \"wb\" ) )\n",
    "#pickle.dump(got_reddit, open(\"got_reddit_ready_vector.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "got = pickle.load( open( \"./Data/got_reddit_ready_vector.pkl\", \"rb\" ) )\n",
    "#asoiaf = pickle.load( open( \"./Data/soiaf_reddit_ready_vector.pkl\", \"rb\" ) )\n",
    "#freefolk = pickle.load( open( \"./Data/freefolk_reddit_ready_vector.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'could rewatch episode one would watch can’t wait season i’m curious see episode would rewatch could watch pick chronological order winter coming baelor blackwater rain castamere mhysa lion rose mountain viper child hardhome mother’s mercy door battle bastard wind winter spoil war dragon wolf hard come list there’s many great episode people limited time budget rewatching hope helpful spark conversation'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "got['clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning each of subreddits separately so we can remove words which appear too frequently and not often enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOT\n",
    "\n",
    "count_dict_GOT = {}\n",
    "\n",
    "for doc in got['clean']:\n",
    "    for word in doc.split():\n",
    "        if word in count_dict_GOT.keys():\n",
    "            count_dict_GOT[word] +=1\n",
    "        else:\n",
    "            count_dict_GOT[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_dict_GOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### These are helper functions not used in the current cleaning process, saved for reference to potentially be used in further cleaning efforts.\n",
    "\n",
    "#low_value = 15\n",
    "#for key in count_dict.keys():\n",
    "    #if count_dict[key] < low_value: \n",
    "#corpus = [doc.split() for doc in got['clean']]\n",
    "#for item in got['clean']:\n",
    "    #got['clean_list'][item] = item.split()\n",
    "    #corpus = [word for word in corpus if word not in bad_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_value = 20\n",
    "bad_words_GOT = [key for key in count_dict_GOT.keys() if count_dict_GOT[key] < low_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [doc.split() for doc in got['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a list of lists where each document is a string broken into a list of words\n",
    "cleaned_corpus_GOT = []\n",
    "for document in corpus:\n",
    "    cleaned_corpus_GOT.append([word for word in document if word not in bad_words_GOT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cleaned_corpus_GOT, open(\"cleaned_corpus_GOT.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stop_words = ['ign', 'diggory', 'cedric', 'timon', 'pumba', 'arnold', 'gotzmilk', 'episode', 'season', 'character', 'show', \n",
    "                    'amp', 'xb', 'de', 'et', 'le', 'http', 'la', 'com', 'il', 'de', 'gt', 'se', 'gt', 'qui', 'lt', 'xb', 'amp',\n",
    "                   'one', 'would', 'could', 'gif', \"youl'\", 'may', 'get', 'instagram', 'aa',\n",
    " 'aaa',\n",
    " 'aaaaaaand',\n",
    " 'aaaaand',\n",
    " 'aaaand',\n",
    " 'aaand',\n",
    " 'aabns',\n",
    " 'aabxbmaea',\n",
    " 'aafdadceaf',\n",
    " 'aarambha',\n",
    " 'aaron',\n",
    " 'aarya',\n",
    " 'ab', 'abc', 'er', 'nk', 'ww' ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "further_cleaned_corpus_GOT = []\n",
    "for document in cleaned_corpus_GOT:\n",
    "    further_cleaned_corpus_GOT.append([word for word in document if word not in extra_stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(further_cleaned_corpus_GOT, open(\"further_cleaned_corpus_GOT.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_GOT = []\n",
    "\n",
    "for item in further_cleaned_corpus_GOT:\n",
    "    item = ' '.join(item)\n",
    "    final_GOT.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_GOT, open(\"final_got.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
